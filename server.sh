python3 -m llama_cpp.server --model "./models/ggml-vistral-7B-chat-q4_0.gguf" --chat_format chatml --n_gpu_layers 1
#python3 -m llama_cpp.server --model "./models/vietnamese-llama2-7b-40gb.Q3_K_L.gguf" --chat_format llama-2 --n_gpu_layers 1
#python3 -m llama_cpp.server --model "./models/mistral-7b-openorca.Q3_K_L.gguf" --chat_format chatml --n_gpu_layers 1
#ggml-vistral-7B-chat-q4_0.gguf